# RL Pricing Agent

A reinforcement learning project that trains PPO agents to learn optimal pricing strategies for a SaaS product.

## Overview

This project implements three versions of a PPO (Proximal Policy Optimization) agent designed to learn pricing strategies in a simulated market environment. The agents interact with different reward structures and penalty systems to optimize revenue generation.

## ğŸ› ï¸ Project Structure

```
refactor/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ environments/
â”‚   â”‚   â”œâ”€â”€ pricing_env.py         # Original environment
â”‚   â”‚   â”œâ”€â”€ pricing_env_v2.py      # Version 2 with penalties
â”‚   â”‚   â””â”€â”€ pricing_env_v3.py      # Version 3 with comprehensive rewards
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ train_ppo.py           # Original PPO training
â”‚   â”‚   â”œâ”€â”€ train_ppo_v2.py        # Version 2 training
â”‚   â”‚   â””â”€â”€ train_ppo_v3.py        # Version 3 training
â”‚   â””â”€â”€ evaluation/
â”‚       â”œâ”€â”€ evaluate.py            # Original evaluation
â”‚       â”œâ”€â”€ evaluate_v2.py         # Version 2 evaluation
â”‚       â”œâ”€â”€ evaluate_v3.py         # Version 3 evaluation
â”‚       â””â”€â”€ test_final.py          # Clean testing script
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ ppo_pricing_agent.pth      # Trained model v1
â”‚   â”œâ”€â”€ ppo_pricing_agent_v2.pth   # Trained model v2
â”‚   â””â”€â”€ ppo_pricing_agent_v3.pth   # Trained model v3
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ pricing_comparison.png           # V1 results
â”‚   â”œâ”€â”€ pricing_comparison_v2.png        # V2 results
â”‚   â””â”€â”€ final_pricing_comparison_v3.png  # V3 results
â”œâ”€â”€ run_experiments.py     # Main script to run experiments
â”œâ”€â”€ requirements.txt       # Python dependencies
â””â”€â”€ README.md             # This file
```

## ğŸš€ Setup

### Prerequisites

- Python 3.8 or higher
- pip

### Installation

1. Clone the repository
2. Create a virtual environment:

   ```bash
   python -m venv env
   source env/bin/activate

   ```

3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

## ğŸ“Š Usage

### Quick Start

Use the main experiment runner (recommended):

```bash
python run_all_experiments.py
```

```bash
# Train the latest version (v3)
python run_experiments.py --version 3 --mode train

# Evaluate with visualization
python run_experiments.py --version 3 --mode evaluate

# Run final comparison test
python run_experiments.py --mode test
```

### Run Individual Scripts

You can also run scripts directly:

```bash
# Train different versions
python src/training/train_ppo_v3.py    # Latest version
python src/training/train_ppo_v2.py    # Version 2
python src/training/train_ppo.py       # Original version

# Evaluate models
python src/evaluation/evaluate_v3.py   # Evaluate v3 with charts
python src/evaluation/test_final.py    # Clean comparison
```

### Command Line Arguments

The main experiment runner supports:

- `--version {1,2,3}`: Choose which version to run (default: 3)
- `--mode {train,evaluate,test}`: Operation mode (default: train)

## ğŸ¯ Environment Details

### Pricing Environment

The environment simulates a SaaS pricing scenario with:

- **Actions**: Decrease price (-$5), Maintain, Increase price (+$5)
- **State Space**: Current price, revenue history, market conditions
- **Reward**: Revenue-based with various penalty/bonus systems across versions
- **Episode Length**: 30 days of simulation

### Version Differences

- **v1**: Simple revenue maximization
- **v2**: Added penalty system for extreme pricing
- **v3**: Comprehensive reward design with efficiency bonuses

## ğŸ“ˆ Model Training

All models use PPO with:

- Learning rate: 5e-4
- 30,000 total timesteps
- GAE lambda: 0.95
- Clip coefficient: 0.2

Trained models are automatically saved in the `models/` directory.

## ğŸ” Results

Training results and comparison charts are saved in the `results/` directory. Each version generates visualization comparing agent performance against random pricing policies.

## ğŸ“‹ Dependencies

Core libraries:

- `gymnasium` - RL environment framework
- `torch` - Neural network training
- `numpy` - Numerical computations
- `matplotlib` - Visualization
- `cleanrl` - PPO implementation base

See `requirements.txt` for complete dependency list with versions.

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
